{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18bcd06",
   "metadata": {},
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek41.do.txt  -->\n",
    "<!-- dom:TITLE: Exercises week 41 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542d6aa",
   "metadata": {},
   "source": [
    "# Exercises week 41\n",
    "**October 4-11, 2024**\n",
    "\n",
    "Date: **Deadline is Friday October 11 at midnight**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80943a15",
   "metadata": {},
   "source": [
    "# Overarching aims of the exercises this week\n",
    "\n",
    "The aim of the exercises this week is to get started with implementing\n",
    "gradient methods of relevance for project 2. This exercise will also\n",
    "be continued next week with the addition of automatic differentation.\n",
    "Everything you develop here will be used in project 2.\n",
    "\n",
    "In order to get started, we will now replace in our standard ordinary\n",
    "least squares (OLS) and Ridge regression codes (from project 1) the\n",
    "matrix inversion algorithm with our own gradient descent (GD) and SGD\n",
    "codes.  You can use the Franke function or the terrain data from\n",
    "project 1. **However, we recommend using a simpler function like**\n",
    "$f(x)=a_0+a_1x+a_2x^2$ or higher-order one-dimensional polynomials.\n",
    "You can obviously test your final codes against for example the Franke\n",
    "function. Automatic differentiation will be discussed next week.\n",
    "\n",
    "You should include in your analysis of the GD and SGD codes the following elements\n",
    "1. A plain gradient descent with a fixed learning rate (you will need to tune it) using the analytical expression of the gradients\n",
    "\n",
    "2. Add momentum to the plain GD code and compare convergence with a fixed learning rate (you may need to tune the learning rate), again using the analytical expression of the gradients.\n",
    "\n",
    "3. Repeat these steps for stochastic gradient descent with mini batches and a given number of epochs. Use a tunable learning rate as discussed in the lectures from week 39. Discuss the results as functions of the various parameters (size of batches, number of epochs etc)\n",
    "\n",
    "4. Implement the Adagrad method in order to tune the learning rate. Do this with and without momentum for plain gradient descent and SGD.\n",
    "\n",
    "5. Add RMSprop and Adam to your library of methods for tuning the learning rate.\n",
    "\n",
    "The lecture notes from weeks 39 and 40 contain more information and code examples. Feel free to use these examples.\n",
    "\n",
    "In summary, you should \n",
    "perform an analysis of the results for OLS and Ridge regression as\n",
    "function of the chosen learning rates, the number of mini-batches and\n",
    "epochs as well as algorithm for scaling the learning rate. You can\n",
    "also compare your own results with those that can be obtained using\n",
    "for example **Scikit-Learn**'s various SGD options.  Discuss your\n",
    "results. For Ridge regression you need now to study the results as functions of  the hyper-parameter $\\lambda$ and \n",
    "the learning rate $\\eta$.  Discuss your results.\n",
    "\n",
    "You will need your SGD code for the setup of the Neural Network and\n",
    "Logistic Regression codes. You will find the Python [Seaborn\n",
    "package](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n",
    "useful when plotting the results as function of the learning rate\n",
    "$\\eta$ and the hyper-parameter $\\lambda$ when you use Ridge\n",
    "regression.\n",
    "\n",
    "We recommend reading chapter 8 on optimization from the textbook of [Goodfellow, Bengio and Courville](https://www.deeplearningbook.org/). This chapter contains many useful insights and discussions on the optimization part of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095d197",
   "metadata": {},
   "source": [
    "# Code examples from week 39 and 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f428decb",
   "metadata": {},
   "source": [
    "## Code with a Number of Minibatches which varies, analytical gradient\n",
    "\n",
    "In the code here we vary the number of mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba38d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "Own inversion\n",
      "[[4.24440517]\n",
      " [2.83396892]]\n",
      "Eigenvalues of Hessian Matrix:[0.31091283 4.39385597]\n",
      "theta from own gd\n",
      "[[4.24440517]\n",
      " [2.83396892]]\n",
      "theta from own sdg\n",
      "[[4.29010188]\n",
      " [2.7603024 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Importing various packages\n",
    "from math import exp, sqrt\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "\n",
    "#while (iter <= Ni... or test)\n",
    "for iter in range(Niterations):\n",
    "    gradients = 2.0/n*X.T @ ((X @ theta)-y)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = Xnew.dot(theta)\n",
    "ypredict2 = Xnew.dot(theta_linreg)\n",
    "\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "# Can you figure out a better way of setting up the contributions to each batch?\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (2.0/M)* xi.T @ ((xi @ theta)-yi)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        theta = theta - eta*gradients\n",
    "print(\"theta from own sdg\")\n",
    "print(theta)\n",
    "\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de04b41a",
   "metadata": {},
   "source": [
    "In the above code, we have use replacement in setting up the\n",
    "mini-batches. The discussion\n",
    "[here](https://sebastianraschka.com/faq/docs/sgd-methods.html) may be\n",
    "useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc1cca",
   "metadata": {},
   "source": [
    "## Momentum based GD\n",
    "\n",
    "The stochastic gradient descent (SGD) is almost always used with a\n",
    "*momentum* or inertia term that serves as a memory of the direction we\n",
    "are moving in parameter space.  This is typically implemented as\n",
    "follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d1f36",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{v}_{t}=\\gamma \\mathbf{v}_{t-1}+\\eta_{t}\\nabla_\\theta E(\\boldsymbol{\\theta}_t) \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47434945",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\boldsymbol{\\theta}_{t+1}= \\boldsymbol{\\theta}_t -\\boldsymbol{v}_{t},\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea5060",
   "metadata": {},
   "source": [
    "where we have introduced a momentum parameter $\\gamma$, with\n",
    "$0\\le\\gamma\\le 1$, and for brevity we dropped the explicit notation to\n",
    "indicate the gradient is to be taken over a different mini-batch at\n",
    "each step. We call this algorithm gradient descent with momentum\n",
    "(GDM). From these equations, it is clear that $\\mathbf{v}_t$ is a\n",
    "running average of recently encountered gradients and\n",
    "$(1-\\gamma)^{-1}$ sets the characteristic time scale for the memory\n",
    "used in the averaging procedure. Consistent with this, when\n",
    "$\\gamma=0$, this just reduces down to ordinary SGD as discussed\n",
    "earlier. An equivalent way of writing the updates is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923628c8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta \\boldsymbol{\\theta}_{t+1} = \\gamma \\Delta \\boldsymbol{\\theta}_t -\\ \\eta_{t}\\nabla_\\theta E(\\boldsymbol{\\theta}_t),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94031c",
   "metadata": {},
   "source": [
    "where we have defined $\\Delta \\boldsymbol{\\theta}_{t}= \\boldsymbol{\\theta}_t-\\boldsymbol{\\theta}_{t-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f0e9c9",
   "metadata": {},
   "source": [
    "## Algorithms and codes for Adagrad, RMSprop and Adam\n",
    "\n",
    "The algorithms we have implemented are well described in the text by [Goodfellow, Bengio and Courville, chapter 8](https://www.deeplearningbook.org/contents/optimization.html).\n",
    "\n",
    "The codes which implement these algorithms are discussed after our presentation of automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92253eff",
   "metadata": {},
   "source": [
    "## Practical tips\n",
    "\n",
    "* **Randomize the data when making mini-batches**. It is always important to randomly shuffle the data when forming mini-batches. Otherwise, the gradient descent method can fit spurious correlations resulting from the order in which data is presented.\n",
    "\n",
    "* **Transform your inputs**. Learning becomes difficult when our landscape has a mixture of steep and flat directions. One simple trick for minimizing these situations is to standardize the data by subtracting the mean and normalizing the variance of input variables. Whenever possible, also decorrelate the inputs. To understand why this is helpful, consider the case of linear regression. It is easy to show that for the squared error cost function, the Hessian of the cost function is just the correlation matrix between the inputs. Thus, by standardizing the inputs, we are ensuring that the landscape looks homogeneous in all directions in parameter space. Since most deep networks can be viewed as linear transformations followed by a non-linearity at each layer, we expect this intuition to hold beyond the linear case.\n",
    "\n",
    "* **Monitor the out-of-sample performance.** Always monitor the performance of your model on a validation set (a small portion of the training data that is held out of the training process to serve as a proxy for the test set. If the validation error starts increasing, then the model is beginning to overfit. Terminate the learning process. This *early stopping* significantly improves performance in many settings.\n",
    "\n",
    "* **Adaptive optimization methods don't always have good generalization.** Recent studies have shown that adaptive methods such as ADAM, RMSPorp, and AdaGrad tend to have poor generalization compared to SGD or SGD with momentum, particularly in the high-dimensional limit (i.e. the number of parameters exceeds the number of data points). Although it is not clear at this stage why these methods perform so well in training deep neural networks, simpler procedures like properly-tuned SGD may work as well or better in these applications.\n",
    "\n",
    "Geron's text, see chapter 11, has several interesting discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08209015",
   "metadata": {},
   "source": [
    "## Using Automatic differentation with OLS\n",
    "\n",
    "We conclude the part on optmization by showing how we can make codes\n",
    "for linear regression and logistic regression using **autograd**. The\n",
    "first example shows results with ordinary leats squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f7d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients for OLS\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "def CostOLS(beta):\n",
    "    return (1.0/n)*np.sum((y-X @ beta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "# define the gradient\n",
    "training_gradient = grad(CostOLS)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = training_gradient(theta)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = Xnew.dot(theta)\n",
    "ypredict2 = Xnew.dot(theta_linreg)\n",
    "\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc83f33",
   "metadata": {},
   "source": [
    "## Same code but now with momentum gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2a3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients for OLS\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "def CostOLS(beta):\n",
    "    return (1.0/n)*np.sum((y-X @ beta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x#+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 30\n",
    "\n",
    "# define the gradient\n",
    "training_gradient = grad(CostOLS)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = training_gradient(theta)\n",
    "    theta -= eta*gradients\n",
    "    print(iter,gradients[0],gradients[1])\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "# Now improve with momentum gradient descent\n",
    "change = 0.0\n",
    "delta_momentum = 0.3\n",
    "for iter in range(Niterations):\n",
    "    # calculate gradient\n",
    "    gradients = training_gradient(theta)\n",
    "    # calculate update\n",
    "    new_change = eta*gradients+delta_momentum*change\n",
    "    # take a step\n",
    "    theta -= new_change\n",
    "    # save the change\n",
    "    change = new_change\n",
    "    print(iter,gradients[0],gradients[1])\n",
    "print(\"theta from own gd wth momentum\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef007d0",
   "metadata": {},
   "source": [
    "## But noen of these can compete with Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e498aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Newton's method\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "def CostOLS(beta):\n",
    "    return (1.0/n)*np.sum((y-X @ beta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "beta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(beta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "# Note that here the Hessian does not depend on the parameters beta\n",
    "invH = np.linalg.pinv(H)\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "beta = np.random.randn(2,1)\n",
    "Niterations = 5\n",
    "\n",
    "# define the gradient\n",
    "training_gradient = grad(CostOLS)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = training_gradient(beta)\n",
    "    beta -= invH @ gradients\n",
    "    print(iter,gradients[0],gradients[1])\n",
    "print(\"beta from own Newton code\")\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40292cf3",
   "metadata": {},
   "source": [
    "## Including Stochastic Gradient Descent with Autograd\n",
    "In this code we include the stochastic gradient descent approach discussed above. Note here that we specify which argument we are taking the derivative with respect to when using **autograd**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa819b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using SGD\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = (1.0/n)*training_gradient(y, X, theta)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = Xnew.dot(theta)\n",
    "ypredict2 = Xnew.dot(theta_linreg)\n",
    "\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.show()\n",
    "\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "# Can you figure out a better way of setting up the contributions to each batch?\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        theta = theta - eta*gradients\n",
    "print(\"theta from own sdg\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca466b4",
   "metadata": {},
   "source": [
    "## Same code but now with momentum gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d44a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using SGD\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 100\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = (1.0/n)*training_gradient(y, X, theta)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "change = 0.0\n",
    "delta_momentum = 0.3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+delta_momentum*change\n",
    "        # take a step\n",
    "        theta -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "print(\"theta from own sdg with momentum\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82627f6",
   "metadata": {},
   "source": [
    "## AdaGrad algorithm, taken from [Goodfellow et al](https://www.deeplearningbook.org/contents/optimization.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/adagrad.png, width=600 frac=0.8] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/adagrad.png\" width=\"600\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d3aff0",
   "metadata": {},
   "source": [
    "## Similar (second order function now) problem but now with AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b85aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using AdaGrad and Stochastic Gradient descent\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 1000\n",
    "x = np.random.rand(n,1)\n",
    "y = 2.0+3*x +4*x*x\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "\n",
    "# Value for learning rate\n",
    "eta = 0.01\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-8\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*eta/(delta+np.sqrt(Giter))\n",
    "        theta -= update\n",
    "print(\"theta from own AdaGrad\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ddde38",
   "metadata": {},
   "source": [
    "Running this code we note an almost perfect agreement with the results from matrix inversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15b503",
   "metadata": {},
   "source": [
    "## RMSProp algorithm, taken from [Goodfellow et al](https://www.deeplearningbook.org/contents/optimization.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/rmsprop.png, width=600 frac=0.8] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/rmsprop.png\" width=\"600\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f96d12",
   "metadata": {},
   "source": [
    "## RMSprop for adaptive learning rate with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "888f1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using RMSprop  and Stochastic Gradient descent\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 1000\n",
    "x = np.random.rand(n,1)\n",
    "y = 2.0+3*x +4*x*x# +np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "\n",
    "# Value for learning rate\n",
    "eta = 0.01\n",
    "# Value for parameter rho\n",
    "rho = 0.99\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-8\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "\t# Accumulated gradient\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        Giter = (rho*Giter+(1-rho)*gradients*gradients)\n",
    "\t# Taking the diagonal only and inverting\n",
    "        update = gradients*eta/(delta+np.sqrt(Giter))\n",
    "\t# Hadamard product\n",
    "        theta -= update\n",
    "print(\"theta from own RMSprop\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0860f7",
   "metadata": {},
   "source": [
    "## ADAM algorithm, taken from [Goodfellow et al](https://www.deeplearningbook.org/contents/optimization.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/adam.png, width=600 frac=0.8] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/adam.png\" width=\"600\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a9859",
   "metadata": {},
   "source": [
    "## And finally [ADAM](https://arxiv.org/pdf/1412.6980.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccdd4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using RMSprop  and Stochastic Gradient descent\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 1000\n",
    "x = np.random.rand(n,1)\n",
    "y = 2.0+3*x +4*x*x# +np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "\n",
    "# Value for learning rate\n",
    "eta = 0.01\n",
    "# Value for parameters beta1 and beta2, see https://arxiv.org/abs/1412.6980\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-7\n",
    "iter = 0\n",
    "for epoch in range(n_epochs):\n",
    "    first_moment = 0.0\n",
    "    second_moment = 0.0\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        # Computing moments first\n",
    "        first_moment = beta1*first_moment + (1-beta1)*gradients\n",
    "        second_moment = beta2*second_moment+(1-beta2)*gradients*gradients\n",
    "        first_term = first_moment/(1.0-beta1**iter)\n",
    "        second_term = second_moment/(1.0-beta2**iter)\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        update = eta*first_term/(np.sqrt(second_term)+delta)\n",
    "        theta -= update\n",
    "print(\"theta from own ADAM\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ac988c",
   "metadata": {},
   "source": [
    "## Introducing [JAX](https://jax.readthedocs.io/en/latest/)\n",
    "\n",
    "Presently, instead of using **autograd**, we recommend using [JAX](https://jax.readthedocs.io/en/latest/)\n",
    "\n",
    "**JAX** is Autograd and [XLA (Accelerated Linear Algebra))](https://www.tensorflow.org/xla),\n",
    "brought together for high-performance numerical computing and machine learning research.\n",
    "It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d556d0",
   "metadata": {},
   "source": [
    "### Getting started with Jax, note the way we import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b81d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jax import grad as jax_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42db672",
   "metadata": {},
   "source": [
    "### A warm-up example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98eb2f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    return x**2\n",
    "\n",
    "def analytical_gradient(x):\n",
    "    return 2*x\n",
    "\n",
    "def gradient_descent(starting_point, learning_rate, num_iterations, solver=\"analytical\"):\n",
    "    x = starting_point\n",
    "    trajectory_x = [x]\n",
    "    trajectory_y = [function(x)]\n",
    "\n",
    "    if solver == \"analytical\":\n",
    "        grad = analytical_gradient    \n",
    "    elif solver == \"jax\":\n",
    "        grad = jax_grad(function)\n",
    "        x = jnp.float64(x)\n",
    "        learning_rate = jnp.float64(learning_rate)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        \n",
    "        x = x - learning_rate * grad(x)\n",
    "        trajectory_x.append(x)\n",
    "        trajectory_y.append(function(x))\n",
    "\n",
    "    return trajectory_x, trajectory_y\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, function(x), label=\"f(x)\")\n",
    "\n",
    "descent_x, descent_y = gradient_descent(5, 0.1, 10, solver=\"analytical\")\n",
    "jax_descend_x, jax_descend_y = gradient_descent(5, 0.1, 10, solver=\"jax\")\n",
    "\n",
    "plt.plot(descent_x, descent_y, label=\"Gradient descent\", marker=\"o\")\n",
    "plt.plot(jax_descend_x, jax_descend_y, label=\"JAX\", marker=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f19b5",
   "metadata": {},
   "source": [
    "### A more advanced example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8f5eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = np\n",
    "\n",
    "def function(x):\n",
    "    return x*backend.sin(x**2 + 1)\n",
    "\n",
    "def analytical_gradient(x):\n",
    "    return backend.sin(x**2 + 1) + 2*x**2*backend.cos(x**2 + 1)\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, function(x), label=\"f(x)\")\n",
    "\n",
    "descent_x, descent_y = gradient_descent(1, 0.01, 300, solver=\"analytical\")\n",
    "\n",
    "# Change the backend to JAX\n",
    "backend = jnp\n",
    "jax_descend_x, jax_descend_y = gradient_descent(1, 0.01, 300, solver=\"jax\")\n",
    "\n",
    "plt.scatter(descent_x, descent_y, label=\"Gradient descent\", marker=\"v\", s=10, color=\"red\") \n",
    "plt.scatter(jax_descend_x, jax_descend_y, label=\"JAX\", marker=\"x\", s=5, color=\"black\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
