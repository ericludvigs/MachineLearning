{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18bcd06",
   "metadata": {},
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek41.do.txt  -->\n",
    "<!-- dom:TITLE: Exercises week 41 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542d6aa",
   "metadata": {},
   "source": [
    "# Exercises week 41\n",
    "**October 4-11, 2024**\n",
    "\n",
    "Date: **Deadline is Friday October 11 at midnight**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80943a15",
   "metadata": {},
   "source": [
    "# Overarching aims of the exercises this week\n",
    "\n",
    "The aim of the exercises this week is to get started with implementing\n",
    "gradient methods of relevance for project 2. This exercise will also\n",
    "be continued next week with the addition of automatic differentation.\n",
    "Everything you develop here will be used in project 2.\n",
    "\n",
    "In order to get started, we will now replace in our standard ordinary\n",
    "least squares (OLS) and Ridge regression codes (from project 1) the\n",
    "matrix inversion algorithm with our own gradient descent (GD) and SGD\n",
    "codes.  You can use the Franke function or the terrain data from\n",
    "project 1. **However, we recommend using a simpler function like**\n",
    "$f(x)=a_0+a_1x+a_2x^2$ or higher-order one-dimensional polynomials.\n",
    "You can obviously test your final codes against for example the Franke\n",
    "function. Automatic differentiation will be discussed next week.\n",
    "\n",
    "You should include in your analysis of the GD and SGD codes the following elements\n",
    "1. A plain gradient descent with a fixed learning rate (you will need to tune it) using the analytical expression of the gradients\n",
    "\n",
    "2. Add momentum to the plain GD code and compare convergence with a fixed learning rate (you may need to tune the learning rate), again using the analytical expression of the gradients.\n",
    "\n",
    "3. Repeat these steps for stochastic gradient descent with mini batches and a given number of epochs. Use a tunable learning rate as discussed in the lectures from week 39. Discuss the results as functions of the various parameters (size of batches, number of epochs etc)\n",
    "\n",
    "4. Implement the Adagrad method in order to tune the learning rate. Do this with and without momentum for plain gradient descent and SGD.\n",
    "\n",
    "5. Add RMSprop and Adam to your library of methods for tuning the learning rate.\n",
    "\n",
    "The lecture notes from weeks 39 and 40 contain more information and code examples. Feel free to use these examples.\n",
    "\n",
    "In summary, you should \n",
    "perform an analysis of the results for OLS and Ridge regression as\n",
    "function of the chosen learning rates, the number of mini-batches and\n",
    "epochs as well as algorithm for scaling the learning rate. You can\n",
    "also compare your own results with those that can be obtained using\n",
    "for example **Scikit-Learn**'s various SGD options.  Discuss your\n",
    "results. For Ridge regression you need now to study the results as functions of  the hyper-parameter $\\lambda$ and \n",
    "the learning rate $\\eta$.  Discuss your results.\n",
    "\n",
    "You will need your SGD code for the setup of the Neural Network and\n",
    "Logistic Regression codes. You will find the Python [Seaborn\n",
    "package](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n",
    "useful when plotting the results as function of the learning rate\n",
    "$\\eta$ and the hyper-parameter $\\lambda$ when you use Ridge\n",
    "regression.\n",
    "\n",
    "We recommend reading chapter 8 on optimization from the textbook of [Goodfellow, Bengio and Courville](https://www.deeplearningbook.org/). This chapter contains many useful insights and discussions on the optimization part of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b0f0cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "023512a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=(100, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a_0</th>\n",
       "      <td>2.135532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_1</th>\n",
       "      <td>-0.229976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_2</th>\n",
       "      <td>0.073004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Real coefficients\n",
       "a_0           2.135532\n",
       "a_1          -0.229976\n",
       "a_2           0.073004"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Settings #\n",
    "\"\"\"\n",
    "s = 999\n",
    "np.random.seed(s)\n",
    "\n",
    "Niterations = 1000\n",
    "n = 100 # data points\n",
    "fixed_eta = 0.03\n",
    "Lambda  = 0.01\n",
    "\n",
    "num_features = 3\n",
    "\n",
    "\"\"\"\n",
    "# Data #\n",
    "\"\"\"\n",
    "x = np.linspace(0,2,n)\n",
    "x = x.reshape(n,1) # conform to shape code assumes\n",
    "print(f\"{x.shape=}\")\n",
    "\n",
    "noise = np.random.randn(n,1)\n",
    "coefficients = np.random.randn(3,1)\n",
    "a_0, a_1, a_2 = coefficients\n",
    "y = a_0 + a_1*x + a_2 * x**2\n",
    "\n",
    "# dataframe for nicer display of betas/thetas/coefficients/whatever\n",
    "coeff_df = pd.DataFrame(coefficients, columns=[\"Real coefficients\"], index=[\"a_0\", \"a_1\", \"a_2\"])\n",
    "display(coeff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1a287558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of Hessian Matrix: [10.38415281  0.76122767  0.03183162]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real coefficients</th>\n",
       "      <th>Manual inversion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a_0</th>\n",
       "      <td>2.135532</td>\n",
       "      <td>2.135532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_1</th>\n",
       "      <td>-0.229976</td>\n",
       "      <td>-0.229976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_2</th>\n",
       "      <td>0.073004</td>\n",
       "      <td>0.073004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Real coefficients  Manual inversion\n",
       "a_0           2.135532          2.135532\n",
       "a_1          -0.229976         -0.229976\n",
       "a_2           0.073004          0.073004"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Calculations #\n",
    "\"\"\"\n",
    "# Design matrix including the intercept\n",
    "# No scaling of data and all data used for training \n",
    "X = np.c_[np.ones((n,1)), x, x**2]\n",
    "\n",
    "# Inverting manually\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "coeff_df[\"Manual inversion\"] = theta_linreg\n",
    "\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix: {EigValues}\")\n",
    "\n",
    "# problem-specific learning rate\n",
    "eta = 1.0/np.max(EigValues)\n",
    "\n",
    "display(coeff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb596b72",
   "metadata": {},
   "source": [
    "## Gradient solver class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83bcb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientRegression:\n",
    "    def __init__(self, Lambda=0, learning_rate=0.05, max_iterations=1000, scaling=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        # lambda=0 is equivalent to just using OLS, code runs but user should probably change it\n",
    "        # not important if *not* intending to use any of the models that need it, so only warn later if that is the case\n",
    "        self.Lambda = Lambda\n",
    "        self.scaling = scaling\n",
    "\n",
    "    def OLS_gradient(self, calculated_coefficients, X, y):\n",
    "        gradient = (2.0/self.n_data)*X.T @ (X @ calculated_coefficients - y)\n",
    "        return gradient\n",
    "    \n",
    "    def Ridge_gradient(self, calculated_coefficients, X, y):\n",
    "        gradient = (2.0/self.n_data)*X.T @ (X @ calculated_coefficients - y) + 2*self.Lambda*calculated_coefficients\n",
    "        return gradient\n",
    "    \n",
    "    def Lasso_gradient(self, calculated_coefficients, X, y):\n",
    "        gradient = (2.0/self.n_data)*X.T @ (X @ calculated_coefficients - y) + self.Lambda*np.sign(calculated_coefficients)\n",
    "        return gradient\n",
    "    \n",
    "    def GDfit(self, X, y, model_name = None, Lambda = None):\n",
    "        n_data, num_features = X.shape\n",
    "        # make accessible outside of class\n",
    "        self.n_data = n_data\n",
    "        self.num_features = num_features\n",
    "\n",
    "        # ensure flag is set, it will be changed if the model is right\n",
    "        using_lambda = False\n",
    "        if Lambda is not None:\n",
    "            self.Lambda = Lambda\n",
    "\n",
    "        gradient = np.zeros(num_features)\n",
    "        calculated_coefficients = np.random.randn(num_features,1)\n",
    "\n",
    "        if model_name is None:\n",
    "            raise ValueError(\"Must specify a model to fit with\")\n",
    "        elif model_name == \"OLS\":    \n",
    "            selected_gradient_function = self.OLS_gradient\n",
    "        elif model_name == \"Ridge\":\n",
    "            selected_gradient_function = self.Ridge_gradient\n",
    "            using_lambda = True\n",
    "        elif model_name == \"Lasso\":\n",
    "            selected_gradient_function = self.Lasso_gradient\n",
    "            using_lambda = True\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model name\")\n",
    "        \n",
    "        if using_lambda and self.Lambda == 0:\n",
    "            print(\"Warning: Ridge selected but hyperparameter Lambda is zero, result will be equivalent to OLS\")\n",
    "        \n",
    "        # gradient loop\n",
    "        for iter in range(self.max_iterations):\n",
    "            # Gradient calculation - depends on selected model\n",
    "            gradient = selected_gradient_function(calculated_coefficients, X, y)\n",
    "            # Update calculated coefficients\n",
    "            calculated_coefficients -= self.learning_rate*gradient\n",
    "\n",
    "        # finished coefficient result\n",
    "        self.calculated_coefficients = calculated_coefficients\n",
    "        return calculated_coefficients\n",
    "\n",
    "    def linear_predict(self, X):\n",
    "        try:\n",
    "            calculated_coefficients = self.calculated_coefficients\n",
    "        except:\n",
    "            raise ValueError(\"Coefficients have not been calculated, run a model fit first\")\n",
    "        \n",
    "        y_predicted = X @ calculated_coefficients\n",
    "        if self.scaling:\n",
    "            y_tilde = y_tilde + self.y_mean\n",
    "\n",
    "        return y_predicted\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e72830",
   "metadata": {},
   "source": [
    "## Basic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "850a1ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real coefficients</th>\n",
       "      <th>Manual inversion</th>\n",
       "      <th>OLS parameters</th>\n",
       "      <th>Ridge parameters</th>\n",
       "      <th>Lasso parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a_0</th>\n",
       "      <td>2.135532</td>\n",
       "      <td>2.135532</td>\n",
       "      <td>2.135530</td>\n",
       "      <td>2.113702</td>\n",
       "      <td>2.119033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_1</th>\n",
       "      <td>-0.229976</td>\n",
       "      <td>-0.229976</td>\n",
       "      <td>-0.229971</td>\n",
       "      <td>-0.183946</td>\n",
       "      <td>-0.187461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_2</th>\n",
       "      <td>0.073004</td>\n",
       "      <td>0.073004</td>\n",
       "      <td>0.073002</td>\n",
       "      <td>0.053367</td>\n",
       "      <td>0.053217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Real coefficients  Manual inversion  OLS parameters  Ridge parameters  \\\n",
       "a_0           2.135532          2.135532        2.135530          2.113702   \n",
       "a_1          -0.229976         -0.229976       -0.229971         -0.183946   \n",
       "a_2           0.073004          0.073004        0.073002          0.053367   \n",
       "\n",
       "     Lasso parameters  \n",
       "a_0          2.119033  \n",
       "a_1         -0.187461  \n",
       "a_2          0.053217  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_eta = 0.1\n",
    "lr_lambda = 10**(-3)\n",
    "GD_instance = GradientRegression(Lambda=lr_lambda, max_iterations=10000, learning_rate=eta)\n",
    "for model in [\"OLS\", \"Ridge\", \"Lasso\"]:\n",
    "    beta_linreg = GD_instance.GDfit(X, y, model_name = model)\n",
    "    coeff_df[f\"{model} parameters\"] = beta_linreg\n",
    "\n",
    "display(coeff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93319d13",
   "metadata": {},
   "source": [
    "unless lambda is $10^{-3}$ or smaller, ridge and lasso end up completely wrong regardless of learning rate\n",
    "\n",
    "but then learning rate doesn't matter much\n",
    "\n",
    "also max iterations 1000 instead of 10 000 does not converge and is wrong\n",
    "\n",
    "OLS consistently gives the best results anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc1cca",
   "metadata": {},
   "source": [
    "## Momentum based GD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6277f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18248ddc",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ab61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7070391",
   "metadata": {},
   "source": [
    "## Added Adagrad method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f4918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8415bf4d",
   "metadata": {},
   "source": [
    "## Added RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d66c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "983587eb",
   "metadata": {},
   "source": [
    "## Added ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4c41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
