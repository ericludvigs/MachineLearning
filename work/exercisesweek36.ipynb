{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8dd9a1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek36.do.txt  -->\n",
    "<!-- dom:TITLE: Exercises week 36 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ad120e",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exercises week 36\n",
    "**September 2-6, 2024**\n",
    "\n",
    "Date: **Deadline is Friday September 6 at midnight**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63c849",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Overarching aims of the exercises this week\n",
    "\n",
    "This set of exercises form an important part of the first project. The\n",
    "analytical exercises deal with the material covered last week on the\n",
    "mathematical interpretations of ordinary least squares and of Ridge\n",
    "regression. The numerical exercises can be seen as a continuation of\n",
    "exercise 3 from week 35, with the inclusion of Ridge regression. This\n",
    "material enters also the discussions of the first project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b83732",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1: Analytical exercises\n",
    "\n",
    "The aim here is to derive the expression for the optimal parameters\n",
    "using Ridge regression. Furthermore, using the singular value\n",
    "decomposition, we will analyze the difference between the ordinary\n",
    "least squares approach and Ridge regression.\n",
    "\n",
    "The expression for the standard Mean Squared Error (MSE) which we used to define our cost function and the equations for the ordinary least squares (OLS) method, was given by the\n",
    "optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7500f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\beta}\\in {\\mathbb{R}}^{p}}}\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321cd815",
   "metadata": {
    "editable": true
   },
   "source": [
    "which we can also write as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ca592",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\beta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(y_i-\\tilde{y}_i\\right)^2=\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\vert\\vert_2^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e80d60",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have used the definition of  a norm-2 vector, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b82b98",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_2 = \\sqrt{\\sum_i x_i^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87982f",
   "metadata": {
    "editable": true
   },
   "source": [
    "By minimizing the above equation with respect to the parameters\n",
    "$\\boldsymbol{\\beta}$ we could then obtain an analytical expression for the\n",
    "parameters $\\boldsymbol{\\beta}$.\n",
    "\n",
    "We can add a regularization parameter $\\lambda$ by\n",
    "defining a new cost function to be optimized, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340d633",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\beta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\beta}\\vert\\vert_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87f4a6",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to the Ridge regression minimization problem. One can require as part of the optimization problem \n",
    "that $\\vert\\vert \\boldsymbol{\\beta}\\vert\\vert_2^2\\le t$, where $t$ is\n",
    "a finite number larger than zero. We will not implement that here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b7082d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### a) Expression for Ridge regression\n",
    "\n",
    "Show that the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f3cdb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\mathrm{Ridge}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118cc5e4",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\boldsymbol{I}$ being a $p\\times p$ identity matrix with the constraint that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e829cb2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sum_{i=0}^{p-1} \\beta_i^2 \\leq t,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ce6d32",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $t$ a finite positive number. In the optimization, we will not require that the latter is satisfied.\n",
    "\n",
    "The ordinary least squares result is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24acd9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f898e5c",
   "metadata": {
    "editable": true
   },
   "source": [
    "### b) The singular value decomposition\n",
    "\n",
    "Here we will use the singular value decomposition of an $n\\times p$ matrix $\\boldsymbol{X}$ (our design matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd99565",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0cb9de",
   "metadata": {
    "editable": true
   },
   "source": [
    "to study properties of Ridge regression and ordinary least squares regression.\n",
    "Here $\\boldsymbol{U}$ and $\\boldsymbol{V}$ are orthogonal matrices of dimensions\n",
    "$n\\times n$ and $p\\times p$, respectively, and $\\boldsymbol{\\Sigma}$ is an\n",
    "$n\\times p$ matrix which contains the singular values only. This material was discussed during the lectures of week 35.\n",
    "\n",
    "Show that you can write the \n",
    "OLS solutions in terms of the eigenvectors (the columns) of the orthogonal matrix  $\\boldsymbol{U}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74418c2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{y}}_{\\mathrm{OLS}}=\\boldsymbol{X}\\boldsymbol{\\beta}  = \\sum_{j=0}^{p-1}\\boldsymbol{u}_j\\boldsymbol{u}_j^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0279b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "For Ridge regression, show that the corresponding equation is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc4320",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{y}}_{\\mathrm{Ridge}}=\\boldsymbol{X}\\boldsymbol{\\beta}_{\\mathrm{Ridge}} = \\boldsymbol{U\\Sigma V^T}\\left(\\boldsymbol{V}\\boldsymbol{\\Sigma}^2\\boldsymbol{V}^T+\\lambda\\boldsymbol{I} \\right)^{-1}(\\boldsymbol{U\\Sigma V^T})^T\\boldsymbol{y}=\\sum_{j=0}^{p-1}\\boldsymbol{u}_j\\boldsymbol{u}_j^T\\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda}\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f5acf",
   "metadata": {
    "editable": true
   },
   "source": [
    "with the vectors $\\boldsymbol{u}_j$ being the columns of $\\boldsymbol{U}$ from the SVD of the matrix $\\boldsymbol{X}$. \n",
    "\n",
    "Give an interpretation of the results.  [Section 3.4 of Hastie et al's textbook gives a good discussion of the above results](https://link.springer.com/book/10.1007/978-0-387-84858-7)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8044a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: Adding Ridge Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ebf70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as skl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be0118",
   "metadata": {},
   "source": [
    "\n",
    "This exercise is a continuation of exercise 3 from week 35, see <https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/exercisesweek35.html>. We will use the same function to\n",
    "generate our data set, still staying with a simple function $y(x)$\n",
    "which we want to fit using linear regression, but now extending the\n",
    "analysis to include the Ridge regression method.\n",
    "\n",
    "In this exercise you need to include the same elements from last week, that is\n",
    "1. scale your data by subtracting the mean value from each column in the design matrix.\n",
    "\n",
    "2. perform a split of the data in a training set and a test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0a54b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_test</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.636364</td>\n",
       "      <td>0.605587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.886418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.636364</td>\n",
       "      <td>1.107254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.272727</td>\n",
       "      <td>-0.037842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.030303</td>\n",
       "      <td>1.082745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-2.696970</td>\n",
       "      <td>0.026842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.151515</td>\n",
       "      <td>1.392820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.363636</td>\n",
       "      <td>1.041255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.929791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.393939</td>\n",
       "      <td>0.900714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.151515</td>\n",
       "      <td>0.795673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.424242</td>\n",
       "      <td>0.132333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.842964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.515152</td>\n",
       "      <td>0.817112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.181818</td>\n",
       "      <td>0.315372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.909091</td>\n",
       "      <td>1.571483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.121212</td>\n",
       "      <td>1.078398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.835747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.212121</td>\n",
       "      <td>1.453002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-2.030303</td>\n",
       "      <td>0.175217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x_test    y_test\n",
       "0  -0.636364  0.605587\n",
       "1   0.212121  0.886418\n",
       "2   2.636364  1.107254\n",
       "3  -2.272727 -0.037842\n",
       "4  -0.030303  1.082745\n",
       "5  -2.696970  0.026842\n",
       "6   2.151515  1.392820\n",
       "7   1.363636  1.041255\n",
       "8   0.696970  0.929791\n",
       "9  -0.393939  0.900714\n",
       "10 -0.151515  0.795673\n",
       "11 -1.424242  0.132333\n",
       "12  1.000000  0.842964\n",
       "13 -0.515152  0.817112\n",
       "14 -1.181818  0.315372\n",
       "15  1.909091  1.571483\n",
       "16  1.121212  1.078398\n",
       "17  0.818182  0.835747\n",
       "18  2.212121  1.453002\n",
       "19 -2.030303  0.175217"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input data\n",
    "np.random.seed()\n",
    "\n",
    "n = 100\n",
    "print(f\"number of data points: {n}\")\n",
    "\n",
    "lambdas = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "polynomial_degrees = [5, 10, 15]\n",
    "\n",
    "x_full = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y_full = np.exp(-x_full**2) + 1.5 * np.exp(-(x_full-2)**2) + np.random.normal(0, 0.1, x_full.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_full, y_full, test_size=0.2)\n",
    "\n",
    "# fancier display than numpy print\n",
    "results_frame_train = pd.DataFrame({\"x_train\":x_train.flatten(), \"y_train\":y_train.flatten()})\n",
    "results_frame_test = pd.DataFrame({\"x_test\":x_test.flatten(), \"y_test\":y_test.flatten()})\n",
    "display(results_frame_test)\n",
    "\n",
    "# mean value subtraction is for design matrix, we need the polynomial degree P to make a design matrix\n",
    "# want to use different Ps, so subtract mean later when evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730f2dd",
   "metadata": {},
   "source": [
    "\n",
    "The addition to the analysis this time is the introduction of the hyperparameter $\\lambda$ when introducing Ridge regression.\n",
    "\n",
    "Extend the code from exercise 3 from [week 35](https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/exercisesweek35.html) to include Ridge regression with the hyperparameter $\\lambda$. The optimal parameters $\\hat{\\beta}$ for Ridge regression can be obtained by matrix inversion in a similar way as done for ordinary least squares. You need to add to your code the following equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96c8fc3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\mathrm{Ridge}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90046598",
   "metadata": {
    "editable": true
   },
   "source": [
    "The ordinary least squares result you encoded last week is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca23883",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c131be",
   "metadata": {
    "editable": true
   },
   "source": [
    "Use these results to compute the mean squared error for ordinary least\n",
    "squares and Ridge regression first for a polynomial of degree five\n",
    "with $n=100$ data points and five selected values of\n",
    "$\\lambda=[0.0001,0.001, 0.01,0.1,1.0]$. Compute thereafter the mean\n",
    "squared error for the same values of $\\lambda$ for polynomials of degree ten\n",
    "and $15$. Discuss your results for the training MSE and test MSE with\n",
    "Ridge regression and ordinary least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_matrix(x, P):\n",
    "    X = np.zeros((len(x), P))\n",
    "    for exponent in range(0,P):\n",
    "        X[:,exponent] = x[:,0]**exponent\n",
    "\n",
    "    return X\n",
    "\n",
    "def scale_X_and_y(X, y):\n",
    "    \"\"\"Scale by subtracting mean from each column\"\"\"\n",
    "    y_mean = np.mean(y)\n",
    "    X_mean = np.mean(X,axis=0)\n",
    "\n",
    "    X = X - X_mean\n",
    "    y = y - y_mean\n",
    "    return X, y\n",
    "\n",
    "def linear_regression_model_OLS(X, y):\n",
    "    beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    # from https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapter1.html#linear-regression-basic-elements\n",
    "\n",
    "    return beta\n",
    "\n",
    "def linear_regression_model_Ridge(X, y, lambda_value):\n",
    "    beta = np.linalg.inv( X.T.dot(X) + lambda_value * np.identity(X.shape[1]) ).dot(X.T).dot(y)\n",
    "    # from https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapter1.html#linear-regression-basic-elements\n",
    "\n",
    "    return beta\n",
    "\n",
    "def linear_prediction(X, beta):\n",
    "    y_tilde = X @ beta\n",
    "\n",
    "    return y_tilde   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1, ax2 = plt.subplots(1, 2)\n",
    "ax1.set_xlabel(r'Input data')\n",
    "ax1.set_ylabel(r'$y(x)$')\n",
    "ax1.plot(x_full, y_full, alpha=0.7, lw=2,\n",
    "            label='Analytical answer')\n",
    "ax1.legend()\n",
    "ax1.set_title(f\"Non-scaled data\")\n",
    "\n",
    "ax2.set_xlabel(r'Input data')\n",
    "ax2.set_ylabel(r'$y(x)$')\n",
    "ax2.plot(x_full, y_full, alpha=0.7, lw=2,\n",
    "            label='Train output')\n",
    "ax2.legend()\n",
    "ax2.set_title(f\"Scaled data\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
